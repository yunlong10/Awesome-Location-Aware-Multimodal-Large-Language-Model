# Awesome-RegionLLMs [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

## Models

- **LISA: Reasoning Segmentation via Large Language Model**\
  `arXiv  08/2023` [[paper]](https://arxiv.org/abs/2308.00692) [[code]](https://github.com/dvlab-research/lisa)
  
- **BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs**\
  `arXiv 07/2023` [[paper]](https://arxiv.org/abs/2307.08581) [[code]](https://github.com/magic-research/bubogpt)
  
- **GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest**\
  `arXiv 07/2023` [[paper]](https://arxiv.org/abs/2307.03601) [[code]](https://github.com/jshilong/gpt4roi)

- **Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic**\
  `arXiv 07/2023` [[paper]](https://arxiv.org/abs/2306.15195) [[code]](https://github.com/shikras/shikra)

- **Kosmos-2: Grounding Multimodal Large Language Models to the World**\
  `arXiv 06/2023` [[paper]](https://arxiv.org/abs/2306.14824) [[code]](https://github.com/microsoft/unilm/tree/master/kosmos-2)

- **ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System**\
  `arXiv 04/2023` [[paper]](https://arxiv.org/abs/2304.14407)

- **VideoChat: Chat-Centric Video Understanding**\
  `arXiv 05/2023` [[paper]](https://arxiv.org/abs/2305.06355) [[code]](https://github.com/opengvlab/ask-anything)

- **Caption Anything: Interactive Image Description with Diverse Multimodal Controls**\
  `arXiv 05/2023` [[paper]](https://arxiv.org/abs/2305.02677) [[code]](https://github.com/ttengwang/caption-anything)

- **RegionBLIP: A Unified Multi-modal Pre-training Framework for Holistic and Regional Comprehension**\
  `arXiv 08/2023` [[paper]](https://arxiv.org/abs/2308.02299) [[code]](https://github.com/mightyzau/regionblip)

- **Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities**\
  `arXiv 08/2023` [[paper]](https://arxiv.org/abs/2308.12966) [[code]](https://github.com/qwenlm/qwen-vl)

- **OmniDataComposer: A Unified Data Structure for Multimodal Data Fusion and Infinite Data Generation**\
  `arXiv 08/2023` [[paper]](https://arxiv.org/abs/2308.04126) [[code]](https://github.com/shajiayu1/OmniDataComposer)

## Task & Dataset
- RefCOCO/RefCOCOg/RefCOCO+
- Visual Genome
- Flickr30k
