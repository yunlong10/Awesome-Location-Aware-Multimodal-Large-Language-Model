# Awesome-Location-Aware-Multimodal-Large-Language-Model [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

## Location Aware MLLMs

### Regional Location
- **BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs**\
  `arXiv 07/2023` [[paper]](https://arxiv.org/abs/2307.08581) [[code]](https://github.com/magic-research/bubogpt)
  
- **GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest**\
  `arXiv 07/2023` [[paper]](https://arxiv.org/abs/2307.03601) [[code]](https://github.com/jshilong/gpt4roi)

- **Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic**\
  `arXiv 07/2023` [[paper]](https://arxiv.org/abs/2306.15195) [code]

- **Kosmos-2: Grounding Multimodal Large Language Models to the World**\
  `arXiv 06/2023` [[paper]](https://arxiv.org/abs/2306.14824)

### Point Location

- **Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic**\
  `arXiv 07/2023` [[paper]](https://arxiv.org/abs/2306.15195)

### Temporal Location

- **Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning**\
  `CVPR 2023` [[paper]](https://arxiv.org/abs/2302.14115)

- **ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System**\
  `arXiv 04/2023` [[paper]](https://arxiv.org/abs/2304.14407)

## Task & Dataset
